{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from sklearn import preprocessing\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "from sklearn.ensemble import GradientBoostingClassifier as GBT\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "import pytz\n",
    "import pickle\n",
    "import matplotlib as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#from sklearn.preprocessing import CategoricalEncoder\n",
    "#CategoricalEncoder is part of sklearn's developer version, which you can't just update with conda. If you have issues\n",
    "#getting this version, try a hard code implementation of the library here - https://pastebin.com/qs1es9XE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#reads a weird json, and returns the bids dataframe and ads dataframe\n",
    "def read_weird_json(path):\n",
    "    bids = []\n",
    "    ads = []\n",
    "\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            line_dict = json.loads(line.encode('utf-8'))\n",
    "            if 'advertiser_id' in line_dict:\n",
    "                if line_dict['rate_metric'] != 'CPC':\n",
    "                    continue\n",
    "                ads.append(line_dict)\n",
    "            else:\n",
    "                bids.append(line_dict)\n",
    "    df_bids = pd.DataFrame.from_records(bids)\n",
    "    df_ads = pd.DataFrame.from_records(ads)  \n",
    "    \n",
    "    return [df_bids, df_ads]\n",
    "\n",
    "#Returns a list of dataframes. Only looks at ads. \n",
    "def read_many_jsons(paths): \n",
    "    dfs = []\n",
    "    for path in paths: \n",
    "        dfs += [read_weird_json(path)[1]]\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in the data for the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((350926, 39), (368002, 39), (604190, 39), (604190, 39))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Open our hdf files\n",
    "neg2 = pd.HDFStore('./data/day2_negatives_processed.h5')\n",
    "neg3 = pd.HDFStore('./data/day3_negatives_processed.h5')\n",
    "# neg4 = pd.HDFStore('./data/day4_negatives_processed.h5')\n",
    "\n",
    "pos2 = pd.HDFStore('./data/day2_positives_processed.h5')\n",
    "pos3 = pd.HDFStore('./data/day3_positives_processed.h5')\n",
    "# pos4 = pd.HDFStore('./data/day4_positives_processed.h5')\n",
    "\n",
    "#Load out dataframes\n",
    "df_neg2 = neg2['df']\n",
    "df_neg3 = neg3['df']\n",
    "# df_neg4 = neg4['df']\n",
    "\n",
    "df_pos2 = pos2['df']\n",
    "df_pos3 = pos3['df']\n",
    "# df_pos4 = pos4['df']\n",
    "\n",
    "#Close our hdf files\n",
    "neg2.close()\n",
    "neg3.close()\n",
    "# neg4.close()\n",
    "\n",
    "pos2.close()\n",
    "pos3.close()\n",
    "# pos4.close()\n",
    "\n",
    "df_neg2.shape, df_neg3.shape, df_pos2.shape, df_pos3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in the data for the testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1570215, 46)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.HDFStore('./data/test.h5')\n",
    "test_df = test['df']\n",
    "test.close() \n",
    "test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DO NOT RELOAD DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Concatenates both the positive and negative dataframes together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(718928, 39)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negatives = [df_neg2, df_neg3]\n",
    "\n",
    "neg_df = pd.concat(negatives)\n",
    "\n",
    "neg_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1208380, 39)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positives = [df_pos2, df_pos3]\n",
    "\n",
    "pos_df = pd.concat(positives)\n",
    "\n",
    "pos_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1437856, 39)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_pos = pos_df[:718928]\n",
    "train_df_neg = neg_df\n",
    "trains = [train_df_pos, train_df_neg]\n",
    "train_df = pd.concat(trains)\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1437856, 36), (1570215, 36))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Only keep the columns that are in both the training and testing datasets -- TODO: remove this after processing\n",
    "intersection = np.intersect1d(train_df.columns, test_df.columns)\n",
    "\n",
    "train_df = train_df[intersection]\n",
    "test_df = test_df[intersection]\n",
    "train_df.shape, test_df.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_df = pd.concat([train_df, test_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DO NOT RUN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DO NOT RUN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DO NOT RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_map = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One hot encoding function needs to be change - TODO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_column(df, col, thresh=200):\n",
    "    if col in numerical_features:\n",
    "        print(\"Numerical\" , col)\n",
    "        feature_map.append(col)\n",
    "        return df[col].values.reshape(-1,1)\n",
    "    \n",
    "    print(col, df[col].nunique())\n",
    "    \n",
    "    if df[col].nunique() > thresh:\n",
    "        df_frequency = df[[col, 'c_cnt']].groupby(col).agg('count').sort_values('c_cnt',ascending=False)\n",
    "#         print(\"Total len: \", len(df_frequency))\n",
    "#         print(\"SORTED: \", sorted(df_frequency))\n",
    "#         print(\"SORTED INDEXED: \", sorted(df_frequency.iloc[0:thresh].index.values))\n",
    "        cat = [sorted(df_frequency.iloc[0:thresh].index.values)]\n",
    "        dict2 = {}\n",
    "        for i, item in enumerate(cat[0]):\n",
    "            feature_map.append(col + \" - \" + str(item))\n",
    "            dict2[item] = i\n",
    "        #enc = CategoricalEncoder(categories=[sorted(df_frequency[0:thresh].index.values)],handle_unknown='ignore')\n",
    "    else:\n",
    "        dict2 = {}\n",
    "        i = 0\n",
    "        for item in df[col].values:\n",
    "            if item not in dict2:\n",
    "                feature_map.append(col + \" - \" + str(item))\n",
    "                dict2[item] = i\n",
    "                i+=1\n",
    "        #enc = CategoricalEncoder(categories='auto',handle_unknown='ignore')\n",
    "    return [[1 if j == i else 0 for j in dict2] for i in df[col].values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to do some data cleaning. From some initial exploratory analysis, we can see that that we have 5 features with only 16 non-nan values, with a few other features having a similarly low level of non-nan values. To simplify things, we choose to drop all features with less than some threshhold of non-nan values. Also, as we are trying to predict c_cnt, samples where c_cnt is NaN are useless, so we throw those away as well. \n",
    "\n",
    "After this, we see that less than 10% of our remaining samples contains any NaN values, so we just drop those samples as we don't lose that much information from them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_host                  False\n",
      "ad_network_id          False\n",
      "advertiser_id          False\n",
      "c_cnt                  False\n",
      "c_flag_cnt             False\n",
      "campaign_id            False\n",
      "campaign_type          False\n",
      "f_cnt                  False\n",
      "geo_city_name          False\n",
      "geo_country_code3      False\n",
      "geo_region_name        False\n",
      "geo_timezone           False\n",
      "i_cnt                  False\n",
      "i_flag_cnt             False\n",
      "i_timestamp            False\n",
      "pub_network_id         False\n",
      "r_cnt                  False\n",
      "r_num_ads_requested    False\n",
      "r_num_ads_returned     False\n",
      "r_timestamp            False\n",
      "rate_metric            False\n",
      "referer                False\n",
      "session_id             False\n",
      "site_id                False\n",
      "token                  False\n",
      "ua                     False\n",
      "ua_device              False\n",
      "ua_device_type         False\n",
      "ua_major               False\n",
      "ua_minor               False\n",
      "ua_os_name             False\n",
      "url                    False\n",
      "user_agent             False\n",
      "uuid                   False\n",
      "vi_cnt                 False\n",
      "vi_flag_cnt            False\n",
      "dtype: bool\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3008071, 36)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(total_df.isna().any())\n",
    "# print(train_df.count())\n",
    "n = len(total_df)\n",
    "\n",
    "#filter rows with c_cnt as NaN\n",
    "total_df = total_df[np.isfinite(total_df['c_cnt'])]\n",
    "\n",
    "#filter threshhold\n",
    "total_df = total_df.dropna(thresh=int(0.5*n), axis=1)\n",
    "\n",
    "#drop all samples with NaN values\n",
    "total_df = total_df.dropna(axis=0)\n",
    "\n",
    "total_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have some more preprocessing to do, so we wrote some simple functions for preprocessing. The most important thing we do here is that since most of our features are categorical, we must encode them with one-hot-encoding, which essentially turns one feature into n different features, one for each type of class in the original features. For example, if we had a feature for \"hair color\", we would map it to a higher dimensional feature space consisting of \"is the hair white\", \"is the hair black\", \"is the hair brown\", etc. Only one of these features would be a 1, and the rest would be 0.\n",
    "\n",
    "Normally, each feature would be mapped to n features, with n being the number of unique classes that feature contains. For our data, however, some features will have thousands, even millions of unique classes, which would result is an omega-sparse dataset. To account for this, we set a threshhold at 200, such that n will never be greater than 201. We still keep track of the 200 most frequent classes, however, the rest will be bunched into a single class. The motivation for this is that for the more frequent classes, we have enough data that our ML models will be able to extract some information, but for the less frequent classes, there is too little data for accurate analysis, so we group them as one class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Turns a timestamp into which minute the time was at - used as a categorical feature.\n",
    "def timestamp_to_min(timestamp, is_hour=True):\n",
    "    if is_hour:\n",
    "        return timestamp.split(':')[0][-2:]\n",
    "    else: \n",
    "        return timestamp.split(':')[1]\n",
    "\n",
    "#plots frequency of a feature's different classes, useful for exploratory analysis\n",
    "def plot_freq(col_name, df):\n",
    "    df_frequency = df.groupby(col_name).agg('count').sort_values('ad_type',ascending=False)\n",
    "    plt.plot([i for i in range(len(df_frequency.values))], [np.log(i[2]) for i in df_frequency.values])\n",
    "    plt.show()\n",
    "\n",
    "#if a feature only has one unique value, it tells us nothing, so we drop it.\n",
    "def remove_only_ones(df):\n",
    "    for col in df.columns:\n",
    "        if len(df[col].unique()) == 1:\n",
    "            print(col, \" \", df[col].unique())\n",
    "            df.drop(col, inplace=True,axis=1)\n",
    "\n",
    "#just prints how many unique values are in each feature\n",
    "def print_column_counts(df):    \n",
    "    for i in df:\n",
    "        print(i, df[i].nunique())\n",
    "\n",
    "#We do some final cleaning, changing all non-numerical features into strings for later.\n",
    "def preprocess(df):    \n",
    "    for i in df:\n",
    "        if i[-1] != 't' or i[-2] != 'n' or i[-3] != 'c':\n",
    "            df[i] = df[i].astype('str')\n",
    "    remove_only_ones(df)\n",
    "#     print(\"2: \", df.columns)\n",
    "    if 'site_id' in df.columns:\n",
    "        df.drop('site_id',inplace=True,axis=1)\n",
    "    df['i_timestamp'] = df['i_timestamp'].apply(timestamp_to_min)\n",
    "    df['r_timestamp'] = df['r_timestamp'].apply(timestamp_to_min)\n",
    "    \n",
    "\n",
    "# print(train_df.c_cnt)\n",
    "# print(\"1: \", train_df.columns)\n",
    "#final preprocessing\n",
    "# preprocess(train_df)\n",
    "# preprocess(test_df)\n",
    "preprocess(total_df)\n",
    "\n",
    "# print(\"3: \", train_df.columns)\n",
    "#this set contains our numerical column names\n",
    "numerical_features = set(['c_cnt', 'i_cnt', 'r_cnt', 'vi_cnt'])\n",
    "#we create a copy so that X will not include 'c_cnt'\n",
    "# df2_train = train_df.copy()\n",
    "# df2_train.drop('c_cnt',inplace=True,axis=1)\n",
    "\n",
    "# df2_test = test_df.copy()\n",
    "# df2_test.drop('c_cnt',inplace=True,axis=1)\n",
    "df2 = total_df.copy()\n",
    "to_drop = ['c_cnt', 'cr_cnt', 'i_cnt', 'vi_cnt', 'r_num_ads_returned', 'i_flag_cnt', 'vi_flag_cnt']\n",
    "for need_drop in to_drop: \n",
    "    if need_drop in df2.columns:\n",
    "        df2.drop(need_drop,inplace=True,axis=1)\n",
    "#u,s,v = np.linalg.svd(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fix_class_imbalance_with_subsampling(tempX, tempY, pos_ratio=2):\n",
    "    tempY = tempY.reshape(-1,1)\n",
    "    ind_1, ind_0 = [], []\n",
    "    for i, y_h in enumerate(tempY):\n",
    "        if y_h: ind_1.append(i)\n",
    "        else: ind_0.append(i)\n",
    "    to_sample = np.random.permutation(pos_ratio*len(ind_1))\n",
    "    to_sample_0 = [ind_0[i] for i in to_sample]\n",
    "    X2 = np.vstack([tempX[ind_1],tempX[to_sample_0]])\n",
    "    Y2 = np.vstack([tempY[ind_1],tempY[to_sample_0]])\n",
    "    tempY = tempY.reshape(-1)\n",
    "    \n",
    "    new_ind = np.random.permutation(len(X2))\n",
    "    return X2[new_ind],Y2[new_ind]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#given a categorical column, we apply our earlier strategy of one-hot-encoding with maximum thresh=200\n",
    "def transform_column(df, col, thresh=200, return_labels=False):\n",
    "    print(col)\n",
    "    df_frequency = df[[col, 'c_cnt']].groupby(col).agg('count').sort_values('c_cnt',ascending=False)\n",
    "    if df[col].nunique() > thresh:\n",
    "        enc = CategoricalEncoder(categories=[sorted(df_frequency[0:thresh].index.values)],handle_unknown='ignore')\n",
    "        labels = df_frequency[0:thresh].index.values\n",
    "    else:\n",
    "        enc = CategoricalEncoder(categories=[sorted(df_frequency.index.values)],handle_unknown='ignore')\n",
    "        labels = df_frequency.index.values\n",
    "    labels = [str(col) + str(i) for i in labels]\n",
    "    if return_labels:\n",
    "        return labels\n",
    "    enc.fit(df[col].values.reshape(-1, 1))\n",
    "    return enc.transform(df[col].values.reshape(-1,1)).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create our X and Y matrices - adjust threshhold values for 1HE here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_host 145\n",
      "ad_network_id 81\n",
      "advertiser_id 147\n",
      "c_flag_cnt 33\n",
      "campaign_id 812\n",
      "campaign_type 5\n",
      "f_cnt 2\n",
      "geo_city_name 38663\n",
      "geo_country_code3 213\n",
      "geo_region_name 388\n",
      "geo_timezone 295\n",
      "i_timestamp 33\n",
      "pub_network_id 16\n",
      "r_num_ads_requested 9\n",
      "r_timestamp 33\n",
      "rate_metric 2\n",
      "referer 176088\n",
      "session_id 2788997\n",
      "token 31352\n",
      "ua 79095\n",
      "ua_device 10726\n",
      "ua_device_type 4\n",
      "ua_major 157\n",
      "ua_minor 84\n",
      "ua_os_name 32\n",
      "url 402053\n",
      "user_agent 59409\n",
      "uuid 1802282\n"
     ]
    }
   ],
   "source": [
    "one_hot_thresh = 20\n",
    "\n",
    "Y = total_df['c_cnt'].values\n",
    "\n",
    "# X = np.hstack(tuplelist)\n",
    "\n",
    "# unstacked = [get_unique_with_ohe(total_df, col, thresh=one_hot_thresh) if col not in numerical_features else total_df[col].values.reshape(-1,1) for col in df2]\n",
    "X = np.hstack([transform_column(total_df, col, thresh=one_hot_thresh) if col not in numerical_features else total_df[col].values.reshape(-1,1) for col in df2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3008071, 479), (3008071,))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X[:1437856]\n",
    "X_test = X[1437856:]\n",
    "y_train = Y[:1437856]\n",
    "y_test = Y[1437856:]\n",
    "\n",
    "X_fix , Y_fix = X_train, y_train#fix_class_imbalance_with_subsampling(X_train, y_train, pos_ratio=45)\n",
    "Y_fix=Y_fix.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom training score for various paramters - Inverse of Euclidian Distance: Rewards both higher and less \"extreme\" values. Uses Precision of training as X and Recall as Y. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score(y_pred , y_test):\n",
    "    test = confusion_matrix(y_test , y_pred)\n",
    "    prec = test[1][1] / (test[1][1] + test[0][1])\n",
    "    rec = test[1][1] / (test[1][1] + test[1][0])\n",
    "    print(\"Precision: \", prec)\n",
    "    print(\"Recall\" , rec)\n",
    "    return ((prec ** 0.5) * (rec ** 0.5))**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(X_fix, label=Y_fix)\n",
    "dtest = xgb.DMatrix(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_depth = 10\n",
    "eta = 0.3\n",
    "num_rounds = 10\n",
    "param = {'max_depth':max_depth, 'eta':eta, 'silent':0, 'eval_metric':'logloss', 'objective':'binary:logistic' }\n",
    "bst = xgb.train(param, dtrain, num_rounds, verbose_eval=200)\n",
    "preds = bst.predict(dtest)\n",
    "y_pred = np.array([float(i) for i in np.round(preds)])\n",
    "precision = precision_score(y_test, y_pred, average='binary')\n",
    "recall = recall_score(y_test, y_pred, average='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max depth: 10 Eta: 0.3 Precision: 0.000698395650941 Recall: 0.489947089947\n"
     ]
    }
   ],
   "source": [
    "print(\"Max depth: \" + str(10) + \" Eta: \" + str(0.3) + \" Precision: \" + str(precision) + \" Recall: \" + str(recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[906785, 662485],\n",
       "       [   482,    463]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.000698395650941\n",
      "Recall 0.489947089947\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0003421769168102213"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.80629539,  0.31246102,  0.31833702, ...,  0.78453678,\n",
       "        0.52902198,  0.31833702], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  1.74631000e+05,   3.22829000e+05,   2.27950000e+05,\n",
       "          9.29920000e+04,   1.30292000e+05,   1.35270000e+05,\n",
       "          9.80000000e+01,   3.96778000e+05,   8.93390000e+04,\n",
       "          3.60000000e+01]),\n",
       " array([ 0.04829452,  0.14168129,  0.23506805,  0.32845482,  0.42184158,\n",
       "         0.51522835,  0.60861512,  0.70200188,  0.79538865,  0.88877541,\n",
       "         0.98216218]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAD8CAYAAACLrvgBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.pyplot.hist(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the cells below will take a lot of time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STOP HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_round = 10\n",
    "\n",
    "max_depth_list = [x*10+10 for x in range(3)]\n",
    "eta_list = [x*0.1+0.1 for x in range(9)]\n",
    "for max_depth in max_depth_list: \n",
    "    for eta in eta_list: \n",
    "        param = {'max_depth':max_depth, 'eta':eta, 'silent':0, 'eval_metric':'logloss', 'objective':'binary:logistic' }\n",
    "        bst = xgb.train(param, dtrain, num_round, verbose_eval=200)\n",
    "        preds = bst.predict(dtest)\n",
    "        y_pred = np.array([float(i) for i in np.round(preds)])\n",
    "        precision = precision_score(y_test, y_pred, average='binary')\n",
    "        recall = recall_score(y_test, y_pred, average='binary')\n",
    "        print(\"Max depth: \" + str(max_depth) + \" Eta: \" + str(eta) + \" Precision: \" + str(precision) + \" Recall: \" + str(recall))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python333",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
