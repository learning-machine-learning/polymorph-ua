{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from sklearn import preprocessing\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "from sklearn.ensemble import GradientBoostingClassifier as GBT\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import log_loss\n",
    "%matplotlib inline\n",
    "\n",
    "#from sklearn.preprocessing import CategoricalEncoder\n",
    "#CategoricalEncoder is part of sklearn's developer version, which you can't just update with conda. If you have issues\n",
    "#getting this version, try a hard code implementation of the library here - https://pastebin.com/qs1es9XE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_store = pd.HDFStore('combined_day1.h5')\n",
    "df = df_store['df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2262922, 75)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_map = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_column(df, col, thresh=200):\n",
    "    if col in numerical_features:\n",
    "        print(\"Numerical\" , col)\n",
    "        feature_map.append(col)\n",
    "        return df[col].values.reshape(-1,1)\n",
    "    \n",
    "    print(col, df[col].nunique())\n",
    "   \n",
    "    if df[col].nunique() > thresh:\n",
    "        df_frequency = df[[col, 'c_cnt']].groupby(col).agg('count').sort_values('c_cnt',ascending=False)\n",
    "        cat = [sorted(df_frequency[0:thresh].index.values)]\n",
    "        dict2 = {}\n",
    "        for i, item in enumerate(cat[0]):\n",
    "            feature_map.append(col + \" - \" + str(item))\n",
    "            dict2[item] = i\n",
    "        #enc = CategoricalEncoder(categories=[sorted(df_frequency[0:thresh].index.values)],handle_unknown='ignore')\n",
    "    else:\n",
    "        dict2 = {}\n",
    "        i = 0\n",
    "        for item in df[col].values:\n",
    "            if item not in dict2:\n",
    "                feature_map.append(col + \" - \" + str(item))\n",
    "                dict2[item] = i\n",
    "                i+=1\n",
    "        #enc = CategoricalEncoder(categories='auto',handle_unknown='ignore')\n",
    "    return [[1 if j == i else 0 for j in dict2] for i in df[col].values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to do some data cleaning. From some initial exploratory analysis, we can see that that we have 5 features with only 16 non-nan values, with a few other features having a similarly low level of non-nan values. To simplify things, we choose to drop all features with less than some threshhold of non-nan values. Also, as we are trying to predict c_cnt, samples where c_cnt is NaN are useless, so we throw those away as well. \n",
    "\n",
    "After this, we see that less than 10% of our remaining samples contains any NaN values, so we just drop those samples as we don't lose that much information from them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_host                    2262922\n",
      "ad_network_id            2262922\n",
      "ad_type                  2262922\n",
      "adlog_count              2262922\n",
      "advertiser_id            2262922\n",
      "bid_requests             2262922\n",
      "bid_responses            2262922\n",
      "c_cnt                    1724754\n",
      "c_flag_cnt               1724754\n",
      "c_timestamp                 1186\n",
      "c_txn_fee                    429\n",
      "c_txn_rate                   429\n",
      "campaign_id              2262922\n",
      "campaign_type            2262922\n",
      "ck                       2262922\n",
      "cr_cnt                   2262922\n",
      "creative_id              2262922\n",
      "exp_mode                  475358\n",
      "f_cnt                    1724754\n",
      "f_nfr                          2\n",
      "f_timestamp                    2\n",
      "flag                       22239\n",
      "geo_area_code            1788769\n",
      "geo_city_code            2182133\n",
      "geo_city_name            2183499\n",
      "geo_continent_code       2262921\n",
      "geo_country_code2        2262921\n",
      "geo_country_code3        2262921\n",
      "geo_dma_code             1788139\n",
      "geo_postal_code          2095169\n",
      "                          ...   \n",
      "r_cnt                    2262922\n",
      "r_num_ads_requested      2262922\n",
      "r_num_ads_returned       2262922\n",
      "r_num_ads_third_party    2262922\n",
      "r_timestamp              2262922\n",
      "rate_metric              2262922\n",
      "referer                  2262922\n",
      "session_id               2262922\n",
      "site_id                  2262922\n",
      "tag_value                      2\n",
      "token                    2262922\n",
      "txn_fee                   492050\n",
      "txn_rate                  492050\n",
      "ua                       2262922\n",
      "ua_device                2262922\n",
      "ua_device_type           2242811\n",
      "ua_major                 2250318\n",
      "ua_minor                 2250152\n",
      "ua_name                  2262922\n",
      "ua_os                    1615455\n",
      "ua_os_name               2262922\n",
      "url                      2259137\n",
      "user_agent               1677621\n",
      "uuid                     2262922\n",
      "vi_cnt                   1724754\n",
      "vi_flag_cnt              1724754\n",
      "vi_timestamp              345207\n",
      "vv_cnt                   1724754\n",
      "widget_id                  20292\n",
      "zone_id                  2262922\n",
      "Length: 75, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#how many non-nan values do we have?\n",
    "print(df.count())\n",
    "n = len(df)\n",
    "\n",
    "#filter rows with c_cnt as NaN\n",
    "df = df[np.isfinite(df['c_cnt'])]\n",
    "\n",
    "#filter threshhold\n",
    "df = df.dropna(thresh=int(0.3*n), axis=1)\n",
    "#drop all samples with NaN values\n",
    "df = df.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have some more preprocessing to do, so we wrote some simple functions for preprocessing. The most important thing we do here is that since most of our features are categorical, we must encode them with one-hot-encoding, which essentially turns one feature into n different features, one for each type of class in the original features. For example, if we had a feature for \"hair color\", we would map it to a higher dimensional feature space consisting of \"is the hair white\", \"is the hair black\", \"is the hair brown\", etc. Only one of these features would be a 1, and the rest would be 0.\n",
    "\n",
    "Normally, each feature would be mapped to n features, with n being the number of unique classes that feature contains. For our data, however, some features will have thousands, even millions of unique classes, which would result is an omega-sparse dataset. To account for this, we set a threshhold at 200, such that n will never be greater than 201. We still keep track of the 200 most frequent classes, however, the rest will be bunched into a single class. The motivation for this is that for the more frequent classes, we have enough data that our ML models will be able to extract some information, but for the less frequent classes, there is too little data for accurate analysis, so we group them as one class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Turns a timestamp into which minute the time was at - used as a categorical feature.\n",
    "def timestamp_to_min(timestamp, is_hour=True):\n",
    "    if is_hour:\n",
    "        return timestamp.split(':')[0][-2:]\n",
    "    else: \n",
    "        return timestamp.split(':')[1]\n",
    "\n",
    "#plots frequency of a feature's different classes, useful for exploratory analysis\n",
    "def plot_freq(col_name, df):\n",
    "    df_frequency = df.groupby(col_name).agg('count').sort_values('ad_type',ascending=False)\n",
    "    plt.plot([i for i in range(len(df_frequency.values))], [np.log(i[2]) for i in df_frequency.values])\n",
    "    plt.show()\n",
    "\n",
    "#if a feature only has one unique value, it tells us nothing, so we drop it.\n",
    "def remove_only_ones(df):\n",
    "    for col in df.columns:\n",
    "        if len(df[col].unique()) == 1:\n",
    "            df.drop(col, inplace=True,axis=1)\n",
    "\n",
    "#just prints how many unique values are in each feature\n",
    "def print_column_counts(df):    \n",
    "    for i in df:\n",
    "        print(i, df[i].nunique())\n",
    "\n",
    "#We do some final cleaning, changing all non-numerical features into strings for later.\n",
    "def preprocess(df):    \n",
    "    for i in df:\n",
    "        if i[-1] != 't' or i[-2] != 'n' or i[-3] != 'c':\n",
    "            df[i] = df[i].astype('str')\n",
    "    remove_only_ones(df)\n",
    "    if 'site_id' in df.columns:\n",
    "        df.drop('site_id',inplace=True,axis=1)\n",
    "    df['i_timestamp'] = df['i_timestamp'].apply(timestamp_to_min)\n",
    "    df['r_timestamp'] = df['r_timestamp'].apply(timestamp_to_min)\n",
    "    \n",
    "\n",
    "\n",
    "#final preprocessing\n",
    "preprocess(df)\n",
    "#this set contains our numerical column names\n",
    "numerical_features = set(['c_cnt', 'i_cnt', 'r_cnt', 'vi_cnt'])\n",
    "#we create a copy so that X will not include 'c_cnt'\n",
    "df2 = df.copy()\n",
    "df2.drop('c_cnt',inplace=True,axis=1)\n",
    "#u,s,v = np.linalg.svd(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_removed = df.drop( [ \"i_cnt\", \"vi_cnt\", \"r_num_ads_returned\", \"i_flag_cnt\", \"vi_flag_cnt\"] , axis=1)\n",
    "df2_removed = df_removed.copy()\n",
    "df2_removed.drop('c_cnt',inplace=True,axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df_removed\n",
    "df2 = df2_removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#given a categorical column, we apply our earlier strategy of one-hot-encoding with maximum thresh=200\n",
    "def transform_column(df, col, thresh=200, return_labels=False):\n",
    "    print(col)\n",
    "    df_frequency = df[[col, 'c_cnt']].groupby(col).agg('count').sort_values('c_cnt',ascending=False)\n",
    "    if df[col].nunique() > thresh:\n",
    "        enc = CategoricalEncoder(categories=[sorted(df_frequency[0:thresh].index.values)],handle_unknown='ignore')\n",
    "        labels = df_frequency[0:thresh].index.values\n",
    "    else:\n",
    "        enc = CategoricalEncoder(categories=[sorted(df_frequency.index.values)],handle_unknown='ignore')\n",
    "        labels = df_frequency.index.values\n",
    "    labels = [str(col) + str(i) for i in labels]\n",
    "    if return_labels:\n",
    "        return labels\n",
    "    enc.fit(df[col].values.reshape(-1, 1))\n",
    "    return enc.transform(df[col].values.reshape(-1,1)).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create our X and Y matrices - adjust threshhold values for 1HE here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_host 46\n",
      "ad_network_id 16\n",
      "ad_type 2\n",
      "advertiser_id 23\n",
      "c_flag_cnt 3\n",
      "campaign_id 44\n",
      "campaign_type 2\n",
      "ck 5\n",
      "creative_id 100\n",
      "f_cnt 2\n",
      "geo_area_code 256\n",
      "geo_city_code 11444\n",
      "geo_city_name 8139\n",
      "geo_dma_code 210\n",
      "geo_postal_code 16860\n",
      "geo_region_name 51\n",
      "geo_timezone 8\n",
      "Numerical i_cnt\n",
      "i_flag_cnt 4\n",
      "i_timestamp 11\n",
      "ip_address 260620\n",
      "num_ads 6\n",
      "pub_network_id 3\n",
      "Numerical r_cnt\n",
      "r_num_ads_requested 6\n",
      "r_num_ads_returned 8\n",
      "r_timestamp 13\n",
      "rate_metric 2\n",
      "referer 74092\n",
      "session_id 927350\n",
      "token 100\n",
      "ua 12237\n",
      "ua_device 1724\n",
      "ua_device_type 4\n",
      "ua_major 72\n",
      "ua_minor 31\n",
      "ua_name 35\n",
      "ua_os 65\n",
      "ua_os_name 8\n",
      "url 90728\n",
      "user_agent 12217\n",
      "uuid 475380\n",
      "Numerical vi_cnt\n",
      "vi_flag_cnt 3\n",
      "zone_id 51\n"
     ]
    }
   ],
   "source": [
    "one_hot_thresh = 20\n",
    "Y = df['c_cnt'].values\n",
    "X = np.hstack([transform_column(df, col, thresh=one_hot_thresh) for col in df2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(948596, 589)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fix_class_imbalance_with_subsampling(tempX, tempY, pos_ratio=2):\n",
    "    tempY = tempY.reshape(-1,1)\n",
    "    ind_1, ind_0 = [], []\n",
    "    for i, y_h in enumerate(tempY):\n",
    "        if y_h: ind_1.append(i)\n",
    "        else: ind_0.append(i)\n",
    "    to_sample = np.random.permutation(pos_ratio*len(ind_1))\n",
    "    to_sample_0 = [ind_0[i] for i in to_sample]\n",
    "    X2 = np.vstack([tempX[ind_1],tempX[to_sample_0]])\n",
    "    Y2 = np.vstack([tempY[ind_1],tempY[to_sample_0]])\n",
    "    tempY = tempY.reshape(-1)\n",
    "    \n",
    "    new_ind = np.random.permutation(len(X2))\n",
    "    return X2[new_ind],Y2[new_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score(y_pred , y_test):\n",
    "    test = confusion_matrix(y_test , y_pred)\n",
    "    prec = test[1][1] / (test[1][1] + test[0][1])\n",
    "    rec = test[1][1] / (test[1][1] + test[1][0])\n",
    "    print(\"Precision: \", prec)\n",
    "    print(\"Recall: \" , rec)\n",
    "    print(\"Log Loss: \", log_loss(y_test,y_pred))\n",
    "    print(test)\n",
    "    return f1_score(y_test , y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we split the data into 2 sets: training and testing to avoid overfitting of the model. This is done before any subsampling to avoid contaminating the test set. The train set is now subsampled to increase ratio of clicks to nonclicks from 1:2000 to 1:3 which allows models to more accurately learn the click patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.272727272727\n",
      "Recall:  0.100840336134\n",
      "Log Loss:  0.0168702381624\n",
      "[[284428     32]\n",
      " [   107     12]]\n",
      "F1 Score:  0.147239263804\n",
      "Precision:  0.48\n",
      "Recall:  0.100840336134\n",
      "Log Loss:  0.0145641932896\n",
      "[[284447     13]\n",
      " [   107     12]]\n",
      "F1 Score:  0.166666666667\n",
      "Precision:  0.333333333333\n",
      "Recall:  0.100840336134\n",
      "Log Loss:  0.0158992719001\n",
      "[[284436     24]\n",
      " [   107     12]]\n",
      "F1 Score:  0.154838709677\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, Y, test_size=0.3)\n",
    "\n",
    "for i in [60,80,100]:\n",
    "    X_fix , Y_fix = fix_class_imbalance_with_subsampling(X_train, y_train, pos_ratio=i)\n",
    "    Y_fix=Y_fix.ravel()\n",
    "    model = xgb.XGBClassifier(gamma = 5 , min_child_weight = 3, objective = 'binary:logistic')\n",
    "    model.fit(X_fix ,Y_fix )\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(\"F1 Score: \" , score(y_pred, y_test))\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
